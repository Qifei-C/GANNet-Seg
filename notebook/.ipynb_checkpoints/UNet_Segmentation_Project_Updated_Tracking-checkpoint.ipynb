{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a5147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete. Total subjects: 369, Train: 295, Validation: 74\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Updated dataset CSV path\n",
    "csv_path = \"../data/training_detailed_summary_2020.csv\"\n",
    "save_dir = \"data/train_dev_all/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Parse the new dataset CSV\n",
    "data_records = {}\n",
    "with open(csv_path, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        subj_id = row['Subject ID']\n",
    "        scan_type = row['Scan Type']\n",
    "        file_path = row['File Path']\n",
    "        if subj_id not in data_records:\n",
    "            data_records[subj_id] = {}\n",
    "        data_records[subj_id][scan_type] = file_path\n",
    "\n",
    "# Split data into train and validation sets\n",
    "subject_ids = list(data_records.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(subject_ids)\n",
    "\n",
    "train_split = int(len(subject_ids) * 0.8)\n",
    "train_ids = subject_ids[:train_split]\n",
    "val_ids = subject_ids[train_split:]\n",
    "\n",
    "# Calculate mean and std incrementally to avoid memory issues\n",
    "data_types = ['flair', 't1', 't1ce', 't2']\n",
    "mean_std_dict = {dtype: {'mean': 0.0, 'std': 1.0} for dtype in data_types}\n",
    "\n",
    "for dtype in data_types:\n",
    "    sum_vals = 0\n",
    "    sum_squared_vals = 0\n",
    "    num_voxels = 0\n",
    "\n",
    "    for subj_id in train_ids:\n",
    "        if dtype in data_records[subj_id]:\n",
    "            img_path = data_records[subj_id][dtype]\n",
    "            img = nib.load(img_path).get_fdata(dtype=np.float32)\n",
    "            sum_vals += np.sum(img)\n",
    "            sum_squared_vals += np.sum(img**2)\n",
    "            num_voxels += img.size\n",
    "\n",
    "    mean = sum_vals / num_voxels\n",
    "    variance = (sum_squared_vals / num_voxels) - (mean**2)\n",
    "    std = np.sqrt(variance)\n",
    "\n",
    "    mean_std_dict[dtype]['mean'] = mean\n",
    "    mean_std_dict[dtype]['std'] = std\n",
    "\n",
    "# Save the mean and std dictionary\n",
    "with open(os.path.join(save_dir, 'mean_std_dict.pickle'), 'wb') as f:\n",
    "    pickle.dump(mean_std_dict, f)\n",
    "\n",
    "print(f\"Data preparation complete. Total subjects: {len(subject_ids)}, Train: {len(train_ids)}, Validation: {len(val_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d63eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = self.conv_block(n_channels, 64)\n",
    "        self.down1 = self.down(64, 128)\n",
    "        self.down2 = self.down(128, 256)\n",
    "        self.down3 = self.down(256, 512)\n",
    "        self.down4 = self.down(512, 1024)\n",
    "        self.up1 = self.up(1024, 512)\n",
    "        self.up2 = self.up(512, 256)\n",
    "        self.up3 = self.up(256, 128)\n",
    "        self.up4 = self.up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1['upsample'](x5)\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = self.up1['conv'](x)\n",
    "\n",
    "        x = self.up2['upsample'](x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up2['conv'](x)\n",
    "\n",
    "        x = self.up3['upsample'](x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up3['conv'](x)\n",
    "\n",
    "        x = self.up4['upsample'](x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up4['conv'](x)\n",
    "\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_block(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def down(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            UNet.conv_block(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def up(in_channels, out_channels):\n",
    "        return nn.ModuleDict({\n",
    "            'upsample': nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            'conv': UNet.conv_block(out_channels * 2, out_channels)\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a317d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Segmentation file missing for subject 355. Skipping this subject.\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.7133\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.7105\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.7069\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.7030\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.7002\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6975\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6935\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6896\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6856\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6791\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6762\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6709\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6659\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6509\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6393\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6321\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6259\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.6061\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.5851\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.5426\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.4930\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.4200\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.2079\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.3463\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0954\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1733\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.4103\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.2339\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1538\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1774\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1903\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1779\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1427\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1210\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0676\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0986\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0934\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0789\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0672\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0681\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0745\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0631\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0560\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1077\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0816\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0727\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0457\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0266\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0433\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0301\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.1049\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0142\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0307\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0432\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0327\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0587\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0744\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0478\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0523\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0229\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0245\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0545\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0548\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0415\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0265\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0372\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0471\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0165\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0471\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0178\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0403\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0469\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0457\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0430\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0557\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0565\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0636\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0706\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0357\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0384\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0359\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0593\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0352\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0481\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0456\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0306\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0437\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0441\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0251\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0104\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0730\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0149\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0324\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0654\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0110\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0403\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0588\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0542\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0581\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0583\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0753\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0505\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0262\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0331\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0427\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0649\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0337\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0433\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0506\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0789\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0277\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0514\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0313\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0346\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0342\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0838\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0277\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0298\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0492\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0251\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0394\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0487\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0230\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0360\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0539\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0222\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0642\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0144\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0357\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0645\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0469\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0452\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0304\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0322\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0181\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0492\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0403\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0521\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0336\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0475\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0325\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0621\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0427\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0404\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0539\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0649\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0223\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0388\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0559\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0278\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0244\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0366\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0489\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0352\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0467\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0450\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0177\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0712\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0479\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0554\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0161\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0183\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0605\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0469\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0466\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0169\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0508\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0301\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0417\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0255\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0351\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0338\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0367\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0334\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0237\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0420\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0452\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0244\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0392\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0366\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0332\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0507\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0225\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0623\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0361\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0375\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0144\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0153\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0153\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0258\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0404\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0679\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0410\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0281\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0267\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0355\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0272\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0242\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0342\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0557\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0452\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0297\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0442\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0246\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0664\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0271\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0503\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0670\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0404\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0395\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0334\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0242\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0507\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0354\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0387\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0315\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0282\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0327\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0377\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0309\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0505\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0339\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0499\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0281\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0329\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0266\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0268\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0365\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0388\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0233\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0331\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0307\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0303\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0246\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0323\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0436\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0265\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0225\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0234\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0227\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0429\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0390\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0428\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0196\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0565\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0239\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0259\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0517\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0415\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0677\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0298\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0538\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0232\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0357\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0325\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0209\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0290\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0249\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0203\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0399\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0147\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0424\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0366\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0229\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0326\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0156\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0823\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0284\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0184\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0503\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0177\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0293\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0232\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0128\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0325\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0809\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0221\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0447\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0289\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0415\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0409\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0206\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0421\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0354\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0469\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0176\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0380\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0411\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0388\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0320\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0432\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0340\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0473\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0365\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0213\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0381\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0290\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0405\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0657\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0314\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0327\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0187\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0446\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0396\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0331\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0336\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0209\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0530\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0259\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0307\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0246\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0334\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0462\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0337\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0315\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0396\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0133\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0336\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0456\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0429\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0257\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0187\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0175\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0208\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0484\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0206\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0184\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0427\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0325\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0269\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0061\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0179\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0498\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0542\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0234\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0352\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0251\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0107\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0222\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0213\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0553\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0032\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0137\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0381\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0184\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0179\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0105\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0581\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0256\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0380\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0596\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0091\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0451\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0193\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0354\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0274\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0234\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0433\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0165\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0236\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0526\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0343\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0399\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0250\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0225\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0521\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0152\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0442\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0292\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0196\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0561\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0399\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0134\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0403\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0450\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0107\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0290\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0203\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0227\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0220\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0351\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0333\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0300\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0453\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0257\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0294\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0150\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0091\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0344\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0017\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0530\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0429\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0168\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0113\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0299\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0251\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0256\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0217\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0334\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0303\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0193\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0144\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0240\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0514\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0249\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0402\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0394\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0377\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0257\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0268\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0347\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0383\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0240\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0348\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0190\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0142\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0073\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0344\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0397\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0179\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0143\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0365\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0573\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0440\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0256\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0266\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0445\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0246\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0279\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0356\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0267\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0399\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0353\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0227\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0249\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0309\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0159\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0195\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0197\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0159\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0172\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0542\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0402\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0627\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0501\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0271\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0494\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0111\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0246\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0335\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0238\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0312\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0164\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0240\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0203\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0302\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0298\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0377\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0360\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0232\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0360\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0327\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0398\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0272\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0107\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0259\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0309\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0376\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0278\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0215\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0282\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0357\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0130\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0325\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0213\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0112\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0335\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0734\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0139\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0204\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0066\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0360\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0201\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0367\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0346\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0300\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0086\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0118\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0131\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0307\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0173\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0381\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0574\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0525\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0190\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0347\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0185\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0437\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0185\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0424\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0326\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0214\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0553\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0312\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0282\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0295\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0130\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0339\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0237\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0276\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0218\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0276\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0353\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0205\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0069\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0676\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0172\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0548\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0069\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0333\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0199\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0379\n",
      "Epoch [1/50], Batch [0/4557], Loss: 0.0205\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     74\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     76\u001b[0m         images, targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     77\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3.10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3.10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m, in \u001b[0;36mBrainSegDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflair\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt1ce\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt2\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     37\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_records[subj_id][dtype]\n\u001b[1;32m---> 38\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[0;32m     39\u001b[0m     img_slice \u001b[38;5;241m=\u001b[39m img[:, :, slice_idx]\n\u001b[0;32m     40\u001b[0m     img_slice \u001b[38;5;241m=\u001b[39m (img_slice \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_std_dict[dtype][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_std_dict[dtype][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3.10\\lib\\site-packages\\nibabel\\loadsave.py:109\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m sniff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_klass \u001b[38;5;129;01min\u001b[39;00m all_image_classes:\n\u001b[1;32m--> 109\u001b[0m     is_valid, sniff \u001b[38;5;241m=\u001b[39m \u001b[43mimage_klass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_maybe_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msniff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid:\n\u001b[0;32m    111\u001b[0m         img \u001b[38;5;241m=\u001b[39m image_klass\u001b[38;5;241m.\u001b[39mfrom_filename(filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3.10\\lib\\site-packages\\nibabel\\filebasedimages.py:477\u001b[0m, in \u001b[0;36mFileBasedImage.path_maybe_image\u001b[1;34m(klass, filename, sniff, sniff_max)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sniff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sniff[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m klass\u001b[38;5;241m.\u001b[39m_meta_sniff_len:\n\u001b[0;32m    476\u001b[0m     sniff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sniff \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sniff_meta_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta_sniff_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msniff_max\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msniff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sniff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sniff[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m klass\u001b[38;5;241m.\u001b[39m_meta_sniff_len:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, sniff\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3.10\\lib\\site-packages\\nibabel\\filebasedimages.py:422\u001b[0m, in \u001b[0;36mFileBasedImage._sniff_meta_for\u001b[1;34m(klass, filename, sniff_nbytes, sniff)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Attempt to sniff from metadata location\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImageOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_fname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    423\u001b[0m         binaryblock \u001b[38;5;241m=\u001b[39m fobj\u001b[38;5;241m.\u001b[39mread(sniff_nbytes)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m COMPRESSION_ERRORS \u001b[38;5;241m+\u001b[39m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mEOFError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3.10\\lib\\site-packages\\nibabel\\openers.py:181\u001b[0m, in \u001b[0;36mOpener.__init__\u001b[1;34m(self, fileish, *args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# Clear keep_open hint if it is not relevant for the file type\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeep_open\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobj \u001b[38;5;241m=\u001b[39m opener(fileish, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m fileish\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mme_opened \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "# Updated dataset loader\n",
    "class BrainSegDataset(Dataset):\n",
    "    def __init__(self, data_records, mean_std_dict, subject_ids, is_train=True):\n",
    "        self.data_records = data_records\n",
    "        self.mean_std_dict = mean_std_dict\n",
    "        self.subject_ids = subject_ids\n",
    "        self.is_train = is_train\n",
    "        self.slices = self._prepare_slices()\n",
    "\n",
    "    def _prepare_slices(self):\n",
    "        slices = []\n",
    "        for subj_id in self.subject_ids:\n",
    "            if 'seg' not in self.data_records[subj_id]:\n",
    "                print(f\"Warning: Segmentation file missing for subject {subj_id}. Skipping this subject.\")\n",
    "                continue  # Skip subjects without segmentation files\n",
    "            seg_path = self.data_records[subj_id]['seg']\n",
    "            seg_img = nib.load(seg_path).get_fdata()\n",
    "            num_slices = seg_img.shape[-1]\n",
    "            for i in range(num_slices):\n",
    "                slices.append((subj_id, i))\n",
    "        return slices\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subj_id, slice_idx = self.slices[idx]\n",
    "        images = []\n",
    "        for dtype in ['flair', 't1', 't1ce', 't2']:\n",
    "            img_path = self.data_records[subj_id][dtype]\n",
    "            img = nib.load(img_path).get_fdata()\n",
    "            img_slice = img[:, :, slice_idx]\n",
    "            img_slice = (img_slice - self.mean_std_dict[dtype]['mean']) / self.mean_std_dict[dtype]['std']\n",
    "            images.append(img_slice)\n",
    "        images = np.stack(images, axis=0)\n",
    "        images = torch.tensor(images, dtype=torch.float32)\n",
    "\n",
    "        seg_path = self.data_records[subj_id]['seg']\n",
    "        seg_img = nib.load(seg_path).get_fdata()\n",
    "        seg_slice = seg_img[:, :, slice_idx]\n",
    "        seg_slice = (seg_slice > 0).astype(np.float32)  # Normalize target to binary (0 or 1)\n",
    "        seg_slice = torch.tensor(seg_slice, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "        return {'image': images, 'target': seg_slice}\n",
    "\n",
    "\n",
    "# Parameters\n",
    "batch_size = 10\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "# Prepare datasets and loaders\n",
    "train_dataset = BrainSegDataset(data_records, mean_std_dict, train_ids, is_train=True)\n",
    "val_dataset = BrainSegDataset(data_records, mean_std_dict, val_ids, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = UNet(n_channels=4, n_classes=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        images, targets = batch['image'].to(device), batch['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed in {time.time() - start_time:.2f}s with Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, targets = batch['image'].to(device), batch['target'].to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Validation Loss after Epoch [{epoch+1}/{epochs}]: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"checkpoint/unet_brain_seg.pth\")\n",
    "print(\"Training complete. Model saved to checkpoint/unet_brain_seg.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23dc1a-f948-4b7d-b69b-9da3399f41c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
