{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83929123-4e8b-4dc1-a581-f8a04196d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start visualization process\n",
      "Reading dataset\n",
      "Testing dataset integrity...\n",
      "Sample 0: images shape torch.Size([4, 240, 240]), masks shape torch.Size([240, 240])\n",
      "Sample 1: images shape torch.Size([4, 240, 240]), masks shape torch.Size([240, 240])\n",
      "Sample 2: images shape torch.Size([4, 240, 240]), masks shape torch.Size([240, 240])\n",
      "Sample 3: images shape torch.Size([4, 240, 240]), masks shape torch.Size([240, 240])\n",
      "Sample 4: images shape torch.Size([4, 240, 240]), masks shape torch.Size([240, 240])\n",
      "Read file from checkpoint ../model\\model.pth\n",
      "Model Loaded Successfully\n",
      "Dataset size: 2379, Dataloader batches: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing with model.pth:   0%|                                                               | 0/75 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 visualizations for model model.pth at 'visualizations\\model'\n",
      "Visualization process completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "\n",
    "from scipy.special import erf\n",
    "from skimage.morphology import binary_erosion\n",
    "from skimage.measure import label\n",
    "\n",
    "# ===============================\n",
    "# Logging Configuration\n",
    "# ===============================\n",
    "logging.basicConfig(\n",
    "    filename='visualization.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=logging.INFO  # Set to INFO to capture essential logs\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Model Definitions\n",
    "# ===============================\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Handle padding if necessary\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = TF.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)      # (B, 64, H, W)\n",
    "        x2 = self.down1(x1)   # (B, 128, H/2, W/2)\n",
    "        x3 = self.down2(x2)   # (B, 256, H/4, W/4)\n",
    "        x4 = self.down3(x3)   # (B, 512, H/8, W/8)\n",
    "        x5 = self.down4(x4)   # (B, 1024, H/16, W/16)\n",
    "        x = self.up1(x5, x4)  # (B, 512, H/8, W/8)\n",
    "        x = self.up2(x, x3)   # (B, 256, H/4, W/4)\n",
    "        x = self.up3(x, x2)   # (B, 128, H/2, W/2)\n",
    "        x = self.up4(x, x1)   # (B, 64, H, W)\n",
    "        logits = self.outc(x) # (B, n_classes, H, W)\n",
    "        return logits\n",
    "\n",
    "# ===============================\n",
    "# Dataset Definition with Enhanced Error Handling\n",
    "# ===============================\n",
    "\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, csv_path, crop_coords=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by reading the CSV and preparing slice indices that contain tumors.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file containing dataset information.\n",
    "            crop_coords (tuple, optional): Coordinates for cropping images. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.crop_coords = crop_coords\n",
    "        self.slice_info = self._create_slice_index()\n",
    "\n",
    "    def _create_slice_index(self):\n",
    "        \"\"\"\n",
    "        Creates a list of tuples containing subject IDs and slice indices that have tumor labels.\n",
    "\n",
    "        Returns:\n",
    "            list: List of (subject_id, slice_index) tuples.\n",
    "        \"\"\"\n",
    "        slice_info = []\n",
    "        subjects = self.data['Subject ID'].unique()\n",
    "        for subject in subjects:\n",
    "            subject_data = self.data[self.data['Subject ID'] == subject]\n",
    "            try:\n",
    "                # Paths for each modality and segmentation\n",
    "                flair_path = subject_data[subject_data['Scan Type'] == 'flair']['File Path'].values[0]\n",
    "                seg_path = subject_data[subject_data['Scan Type'] == 'seg']['File Path'].values[0]\n",
    "                \n",
    "                # Load images\n",
    "                flair_nii = nib.load(flair_path)\n",
    "                flair_image = flair_nii.get_fdata().astype(np.float32)\n",
    "                seg_nii = nib.load(seg_path)\n",
    "                seg_mask = seg_nii.get_fdata().astype(np.uint8)\n",
    "                seg_mask[seg_mask == 4] = 3  # Merge label 4 into 3 if necessary\n",
    "                \n",
    "                depth = flair_image.shape[2]\n",
    "                for z in range(15, depth - 12):\n",
    "                    # Check if the slice has any tumor labels\n",
    "                    seg_slice = seg_mask[:, :, z]\n",
    "                    if np.any(np.isin(seg_slice, [1, 2, 3])):\n",
    "                        slice_info.append((subject, z))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing subject {subject}: {e}\")\n",
    "        return slice_info\n",
    "\n",
    "    @staticmethod\n",
    "    def Img_proc(image, _lambda=-0.8, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Processes the image using a specific transformation pipeline.\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): Input image slice.\n",
    "            _lambda (float, optional): Lambda parameter for transformation. Defaults to -0.8.\n",
    "            epsilon (float, optional): Small value to prevent division by zero. Defaults to 1e-6.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Processed image.\n",
    "        \"\"\"\n",
    "        if np.isnan(image).any() or np.isinf(image).any():\n",
    "            raise ValueError(\"Invalid image values.\")\n",
    "        I_img = image\n",
    "        min_val = np.min(I_img)\n",
    "        max_val = np.max(I_img)\n",
    "        if max_val == min_val:\n",
    "            return np.zeros_like(I_img)\n",
    "        I_img_norm = (I_img - min_val) / (max_val - min_val + epsilon)\n",
    "        max_I_img = np.max(I_img_norm)\n",
    "        IMG1 = (max_I_img / np.log(max_I_img + 1 + epsilon)) * np.log(I_img_norm + 1)\n",
    "        IMG2 = 1 - np.exp(-I_img_norm)\n",
    "        IMG3 = (IMG1 + IMG2) / (_lambda + (IMG1 * IMG2))\n",
    "        IMG4 = erf(_lambda * np.arctan(np.exp(IMG3)) - 0.5 * IMG3)\n",
    "        min_IMG4 = np.min(IMG4)\n",
    "        max_IMG4 = np.max(IMG4)\n",
    "        if max_IMG4 == min_IMG4:\n",
    "            return np.zeros_like(IMG4)\n",
    "        IMG5 = (IMG4 - min_IMG4) / (max_IMG4 - min_IMG4 + epsilon)\n",
    "        return IMG5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and mask for the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image_tensor, mask_tensor)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            subject, z = self.slice_info[idx]\n",
    "            subject_data = self.data[self.data['Subject ID'] == subject]\n",
    "            modalities = ['flair', 't1', 't1ce', 't2']\n",
    "            slices = []\n",
    "            for modality in modalities:\n",
    "                file_path = subject_data[subject_data['Scan Type'] == modality]['File Path'].values[0]\n",
    "                if not os.path.exists(file_path):\n",
    "                    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "                nii = nib.load(file_path)\n",
    "                image = nii.get_fdata().astype(np.float32)\n",
    "                image = (image - np.mean(image)) / (np.std(image) + 1e-6)\n",
    "                if self.crop_coords:\n",
    "                    min_x, max_x, min_y, max_y = self.crop_coords\n",
    "                    image = image[min_x:max_x, min_y:max_y, :]\n",
    "                slice_img = image[:, :, z]\n",
    "                slice_img = self.Img_proc(slice_img)\n",
    "                slices.append(slice_img)\n",
    "            images = np.stack(slices, axis=0)\n",
    "            seg_data = subject_data[subject_data['Scan Type'] == 'seg']\n",
    "            if seg_data.empty:\n",
    "                raise ValueError(f\"Missing segmentation for subject {subject}\")\n",
    "            seg_path = seg_data['File Path'].values[0]\n",
    "            if not os.path.exists(seg_path):\n",
    "                raise FileNotFoundError(f\"Segmentation file not found: {seg_path}\")\n",
    "            seg_nii = nib.load(seg_path)\n",
    "            seg_mask = seg_nii.get_fdata().astype(np.uint8)\n",
    "            seg_mask[seg_mask == 4] = 3\n",
    "            seg_slice = seg_mask[:, :, z]\n",
    "            if self.crop_coords:\n",
    "                seg_slice = seg_mask[min_x:max_x, min_y:max_y, z]\n",
    "            return torch.tensor(images, dtype=torch.float32), torch.tensor(seg_slice, dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in __getitem__ at index {idx}: {e}\")\n",
    "            raise e  # Re-raise the exception to be handled by the DataLoader\n",
    "\n",
    "# ===============================\n",
    "# Visualization Function\n",
    "# ===============================\n",
    "\n",
    "def visualize_segmentation(image, ground_truth, prediction, save_path, slice_idx):\n",
    "    \"\"\"\n",
    "    Visualize the input image, ground truth mask, and predicted mask side by side.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input MRI slice (H, W).\n",
    "        ground_truth (numpy.ndarray): The ground truth segmentation mask (H, W).\n",
    "        prediction (numpy.ndarray): The predicted segmentation mask (H, W).\n",
    "        save_path (str): Directory path to save the visualization.\n",
    "        slice_idx (int): Index of the slice for naming.\n",
    "    \"\"\"\n",
    "    # Define the color map for segmentation\n",
    "    cmap = plt.get_cmap('jet', np.max(ground_truth) - np.min(ground_truth) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(18, 6))  # Increased size for better clarity\n",
    "\n",
    "    # Input Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Ground Truth Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(ground_truth, cmap=cmap, interpolation='none')\n",
    "    plt.title('Ground Truth Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Predicted Mask\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(prediction, cmap=cmap, interpolation='none')\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(save_path, f'slice_{slice_idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# ===============================\n",
    "# Helper Functions\n",
    "# ===============================\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Load model weights from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to load weights into.\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "        device (torch.device): Device to map the model weights.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The model loaded with weights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        logging.info(f\"Checkpoint loaded with weights_only=True from {checkpoint_path}\")\n",
    "    except TypeError:\n",
    "        # If torch.load doesn't support weights_only (older PyTorch versions)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        logging.warning(\"torch.load does not support weights_only. Loaded without it.\")\n",
    "\n",
    "    # Check if it's a DataParallel model\n",
    "    model_keys = list(model.state_dict().keys())\n",
    "    checkpoint_keys = list(checkpoint.keys())\n",
    "\n",
    "    if any(key.startswith(\"module.\") for key in checkpoint_keys) and not any(key.startswith(\"module.\") for key in model_keys):\n",
    "        # Remove 'module.' prefix\n",
    "        checkpoint = {key.replace(\"module.\", \"\"): value for key, value in checkpoint.items()}\n",
    "    elif not any(key.startswith(\"module.\") for key in checkpoint_keys) and any(key.startswith(\"module.\") for key in model_keys):\n",
    "        # Add 'module.' prefix\n",
    "        checkpoint = {\"module.\" + key: value for key, value in checkpoint.items()}\n",
    "\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "    logging.info(f\"Loaded state_dict into the model from {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "def test_dataset(dataset, num_samples=5):\n",
    "    \"\"\"\n",
    "    Manually test the dataset by accessing a few samples.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to test.\n",
    "        num_samples (int): Number of samples to test.\n",
    "    \"\"\"\n",
    "    print(\"Testing dataset integrity...\")\n",
    "    for idx in range(min(num_samples, len(dataset))):\n",
    "        try:\n",
    "            images, masks = dataset[idx]\n",
    "            print(f\"Sample {idx}: images shape {images.shape}, masks shape {masks.shape}\")\n",
    "            logging.info(f\"Sample {idx}: images shape {images.shape}, masks shape {masks.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing sample {idx}: {e}\")\n",
    "            logging.error(f\"Error accessing sample {idx}: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# Main Function\n",
    "# ===============================\n",
    "\n",
    "def main():\n",
    "    print(\"Start visualization process\")\n",
    "    logging.info(\"Visualization process started\")\n",
    "    \n",
    "    csv_path = '../data/selected_test_subject.csv'  # Update this path as needed\n",
    "    model_dir = \"../model\"  # Directory containing model checkpoints\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Reading dataset\")\n",
    "    logging.info(f\"Reading dataset from {csv_path}\")\n",
    "    \n",
    "    # Initialize dataset without cropping coordinates (if needed, set crop_coords)\n",
    "    dataset = BrainSegmentationDataset(csv_path)\n",
    "    \n",
    "    # Test dataset integrity before proceeding\n",
    "    test_dataset(dataset)\n",
    "    \n",
    "    # Initialize DataLoader with num_workers=0 and reduced batch_size to avoid issues\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,         # Adjust as per your system's capability\n",
    "        shuffle=False,\n",
    "        num_workers=0,         # Set to 0 to prevent worker errors\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Directory to save all visualizations\n",
    "    visualization_root = \"visualizations\"\n",
    "    os.makedirs(visualization_root, exist_ok=True)\n",
    "    logging.info(f\"Visualization root directory set at '{visualization_root}'\")\n",
    "    \n",
    "    # Iterate over all checkpoint files in the model directory\n",
    "    for root, _, files in os.walk(model_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pt\") or file.endswith(\".pth\"):\n",
    "                checkpoint_path = os.path.join(root, file)\n",
    "                print(f\"Read file from checkpoint {checkpoint_path}\")\n",
    "                logging.info(f\"Processing checkpoint: {checkpoint_path}\")\n",
    "                \n",
    "                # Initialize the model\n",
    "                model = UNet(n_channels=4, n_classes=4, bilinear=True).to(device)\n",
    "                \n",
    "                try:\n",
    "                    # Load model weights\n",
    "                    model = load_checkpoint(model, checkpoint_path, device)\n",
    "                    print(\"Model Loaded Successfully\")\n",
    "                    logging.info(f\"Model loaded successfully from {checkpoint_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load the Model: {e}\")\n",
    "                    logging.error(f\"Failed to load the model from {checkpoint_path}: {e}\")\n",
    "                    continue  # Skip to the next checkpoint\n",
    "                \n",
    "                model.eval()\n",
    "                print(f\"Dataset size: {len(dataset)}, Dataloader batches: {len(dataloader)}\")\n",
    "                logging.info(f\"Dataset size: {len(dataset)}, Dataloader batches: {len(dataloader)}\")\n",
    "                \n",
    "                # Directory to save visualizations for this model\n",
    "                save_path = os.path.join(visualization_root, os.path.splitext(file)[0])\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                logging.info(f\"Saving visualizations to '{save_path}'\")\n",
    "                \n",
    "                # Counter for saved visualizations\n",
    "                num_visualizations = 5  # Number of slices to visualize per model\n",
    "                saved_visualizations = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for images, masks in tqdm(dataloader, desc=f\"Visualizing with {file}\"):\n",
    "                        images = images.to(device, dtype=torch.float32)\n",
    "                        masks = masks.numpy()  # Shape: (B, H, W)\n",
    "\n",
    "                        outputs = model(images)  # Shape: (B, 4, H, W)\n",
    "                        preds = torch.argmax(outputs, dim=1).cpu().numpy()  # Shape: (B, H, W)\n",
    "\n",
    "                        batch_size = preds.shape[0]\n",
    "                        for i in range(batch_size):\n",
    "                            if saved_visualizations >= num_visualizations:\n",
    "                                break\n",
    "                            \n",
    "                            # Select the first modality (e.g., FLAIR) for visualization\n",
    "                            input_image = images[i, 0].cpu().numpy()  # Assuming modality 0 is FLAIR\n",
    "                            ground_truth = masks[i]\n",
    "                            prediction = preds[i]\n",
    "\n",
    "                            # Visualize and save\n",
    "                            try:\n",
    "                                visualize_segmentation(\n",
    "                                    image=input_image,\n",
    "                                    ground_truth=ground_truth,\n",
    "                                    prediction=prediction,\n",
    "                                    save_path=save_path,\n",
    "                                    slice_idx=saved_visualizations + 1\n",
    "                                )\n",
    "                                saved_visualizations += 1\n",
    "                                logging.info(f\"Saved visualization slice {saved_visualizations} for model {file}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error during visualization: {e}\")\n",
    "                                logging.error(f\"Error during visualization for slice {saved_visualizations + 1} in model {file}: {e}\")\n",
    "\n",
    "                        if saved_visualizations >= num_visualizations:\n",
    "                            break  # Move to the next checkpoint after saving required visualizations\n",
    "                \n",
    "                print(f\"Saved {saved_visualizations} visualizations for model {file} at '{save_path}'\")\n",
    "                logging.info(f\"Saved {saved_visualizations} visualizations for model {file} at '{save_path}'\")\n",
    "\n",
    "    print(\"Visualization process completed\")\n",
    "    logging.info(\"Visualization process completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        logging.critical(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c2b2e-d653-4215-944c-5ced8f1429dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
